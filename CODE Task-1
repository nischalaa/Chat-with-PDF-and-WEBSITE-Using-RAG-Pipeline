***Task1.Chat with PDF Using RAG Pipeline Overview***

pip install --upgrade openai langchain langchain-community
pip install openai==0.28
import PyPDF2
import os
import faiss
import numpy as np
import openai
from sentence_transformers import SentenceTransformer
from langchain.llms import OpenAI
from langchain_community.llms import OpenAI
from langchain.prompts import PromptTemplate


# Step 1: PDF Text Extraction
def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() if page.extract_text() else ""
    return text

# Step 2: Chunking the Text
def chunk_text(text, chunk_size=200, overlap=50):
    """Split text into chunks of specified size with overlap."""
    chunks = []
    words = text.split()
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i : i + chunk_size])
        chunks.append(chunk)
    return chunks

# Step 3: Generate Embeddings
def generate_embeddings(chunks, model_name="all-MiniLM-L6-v2"):
    """Convert text chunks into embeddings using SentenceTransformer."""
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, show_progress_bar=True)
    return model, embeddings

# Step 4: Store Embeddings in FAISS Vector Database
def build_faiss_index(embeddings):
    """Create a FAISS index and store embeddings."""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity
    index.add(embeddings)  # Add embeddings to the index
    return index

# Step 5: Query Handling
def query_pdf(query, model, faiss_index, chunks, top_k=3):
    """Process user query and return relevant chunks."""
    query_embedding = model.encode([query])
    distances, indices = faiss_index.search(np.array(query_embedding), top_k)
    results = [chunks[idx] for idx in indices[0]]
    return results

# Step 6: Generate Response Using OpenAI GPT
# OpenAI API Key
openai.api_key = "YOUR_OPENAI_API_KEY" # Replace with your OpenAI API key
def generate_response(query, retrieved_chunks):
    """Generate a response using OpenAI GPT."""
    context = "\n\n".join(retrieved_chunks)
    prompt = f"Answer the question based on the following context:\n\n{context}\n\nQuestion: {query}\nAnswer:"

    # Call OpenAI GPT-4 API
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']



# Step 7: Main Workflow
def main(pdf_path, query):
    # Extract and process PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)
    print("Chunking text...")
    chunks = chunk_text(text)
    print(f"Generated {len(chunks)} chunks.")

    # Generate embeddings and build FAISS index
    print("Generating embeddings...")
    model, embeddings = generate_embeddings(chunks)
    print("Building FAISS index...")
    faiss_index = build_faiss_index(embeddings)

    # Query handling
    print("Retrieving relevant chunks...")
    retrieved_chunks = query_pdf(query, model, faiss_index, chunks)
    print("Generating response...")
    response = generate_response(query, retrieved_chunks)

    # Output response
    print("\nResponse:")
    print(response)

if __name__ == "__main__":
    # User input
    pdf_path = "urban affairs.pdf"  # Path to your PDF file
    query = input("Enter your question: ")

    main(pdf_path, query)


user input:
 Enter your question: What is the unemployment rate for individuals with an associate's degree?
OUTPUT:
Response:
The unemployment rate for individuals with an associate's degree is 4.2% as per the data in the document.
-------------------------------------------------------------------------------------------------------------------------------


