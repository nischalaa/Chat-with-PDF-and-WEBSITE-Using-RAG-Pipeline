***Task1.Chat with PDF Using RAG Pipeline Overview***

pip install --upgrade openai langchain langchain-community
pip install openai==0.28
import PyPDF2
import os
import faiss
import numpy as np
import openai
from sentence_transformers import SentenceTransformer
from langchain.llms import OpenAI
from langchain_community.llms import OpenAI
from langchain.prompts import PromptTemplate


# Step 1: PDF Text Extraction
def extract_text_from_pdf(pdf_path):
    """Extract text from a PDF file."""
    text = ""
    with open(pdf_path, "rb") as file:
        reader = PyPDF2.PdfReader(file)
        for page in reader.pages:
            text += page.extract_text() if page.extract_text() else ""
    return text

# Step 2: Chunking the Text
def chunk_text(text, chunk_size=200, overlap=50):
    """Split text into chunks of specified size with overlap."""
    chunks = []
    words = text.split()
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i : i + chunk_size])
        chunks.append(chunk)
    return chunks

# Step 3: Generate Embeddings
def generate_embeddings(chunks, model_name="all-MiniLM-L6-v2"):
    """Convert text chunks into embeddings using SentenceTransformer."""
    model = SentenceTransformer(model_name)
    embeddings = model.encode(chunks, show_progress_bar=True)
    return model, embeddings

# Step 4: Store Embeddings in FAISS Vector Database
def build_faiss_index(embeddings):
    """Create a FAISS index and store embeddings."""
    dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(dimension)  # L2 distance for similarity
    index.add(embeddings)  # Add embeddings to the index
    return index

# Step 5: Query Handling
def query_pdf(query, model, faiss_index, chunks, top_k=3):
    """Process user query and return relevant chunks."""
    query_embedding = model.encode([query])
    distances, indices = faiss_index.search(np.array(query_embedding), top_k)
    results = [chunks[idx] for idx in indices[0]]
    return results

# Step 6: Generate Response Using OpenAI GPT
# OpenAI API Key
openai.api_key = "YOUR_OPENAI_API_KEY" # Replace with your OpenAI API key
def generate_response(query, retrieved_chunks):
    """Generate a response using OpenAI GPT."""
    context = "\n\n".join(retrieved_chunks)
    prompt = f"Answer the question based on the following context:\n\n{context}\n\nQuestion: {query}\nAnswer:"

    # Call OpenAI GPT-4 API
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    )
    return response['choices'][0]['message']['content']



# Step 7: Main Workflow
def main(pdf_path, query):
    # Extract and process PDF
    print("Extracting text from PDF...")
    text = extract_text_from_pdf(pdf_path)
    print("Chunking text...")
    chunks = chunk_text(text)
    print(f"Generated {len(chunks)} chunks.")

    # Generate embeddings and build FAISS index
    print("Generating embeddings...")
    model, embeddings = generate_embeddings(chunks)
    print("Building FAISS index...")
    faiss_index = build_faiss_index(embeddings)

    # Query handling
    print("Retrieving relevant chunks...")
    retrieved_chunks = query_pdf(query, model, faiss_index, chunks)
    print("Generating response...")
    response = generate_response(query, retrieved_chunks)

    # Output response
    print("\nResponse:")
    print(response)

if __name__ == "__main__":
    # User input
    pdf_path = "urban affairs.pdf"  # Path to your PDF file
    query = input("Enter your question: ")

    main(pdf_path, query)


user input:
 Enter your question: What is the unemployment rate for individuals with an associate's degree?
OUTPUT:
Response:
The unemployment rate for individuals with an associate's degree is 4.2% as per the data in the document.
-------------------------------------------------------------------------------------------------------------------------------
***Task2: Chat with Website Using RAG Pipeline*** 

--Data Scraping and Embedding--

import requests
from bs4 import BeautifulSoup
from sentence_transformers import SentenceTransformer
import faiss

# Define the model for embeddings
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# Function to scrape a website
def scrape_website(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # Extract text (simplified version)
    paragraphs = soup.find_all('p')
    text_chunks = [p.get_text() for p in paragraphs]
    
    return text_chunks

# Convert text chunks to embeddings
def get_embeddings(text_chunks):
    embeddings = embedding_model.encode(text_chunks)
    return embeddings

# Store embeddings in FAISS
def store_embeddings(embeddings):
    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings)
    return index

# Example usage
url = "https://www.stanford.edu/"
chunks = scrape_website(url)
embeddings = get_embeddings(chunks)
index = store_embeddings(embeddings)

--Query Handling and Response Generation--

from sklearn.metrics.pairwise import cosine_similarity

# Function to convert query into embedding
def query_to_embedding(query):
    return embedding_model.encode([query])

# Function to perform similarity search
def search_similar_chunks(query_embedding, index, top_n=5):
    D, I = index.search(query_embedding, top_n)  # D is distance, I is indices of nearest neighbors
    return I

# Generate response using LLM (simulated here)
def generate_response(query, index, chunks):
    query_embedding = query_to_embedding(query)
    similar_chunk_indices = search_similar_chunks(query_embedding, index)
    relevant_chunks = [chunks[i] for i in similar_chunk_indices[0]]
    
    # Simulate response generation (using relevant chunks)
    response = f"Based on the relevant information: {relevant_chunks[:2]}"
    return response

# Example usage
query = "What is the history of AI?"
response = generate_response(query, index, chunks)
print(response)

OUTPUT:
Based on the relevant information: ['Driving discoveries vital to our world, our health, and our intellectual life', 'Stanford Arts'].



